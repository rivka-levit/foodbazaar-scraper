services:
  scraper:
    build:
      context: .
      dockerfile: scraper/Dockerfile
    volumes:
      - ./scraper:/scraper
      - dev-uploads-xlsx:/vol/uploads/files
    ports:
      - "80:80"
    env_file:
      - .env
    depends_on:
      chrome:
        condition: service_healthy
      db:
        condition: service_started
    stdin_open: true
    tty: true

  chrome:
    image: selenium/standalone-chrome
    hostname: chrome
    ports:
      - "4444:4444"
    privileged: true
    shm_size: 2g
    healthcheck:
      test: [ "CMD", "wget", "--spider", "http://chrome:4444" ]
      interval: 5s
      timeout: 3s
      retries: 3

  db:
    image: mysql:9
    volumes:
      - dev-db-data:/var/lib/mysql
      - dev-uploads-xlsx:/vol/uploads/files
    env_file:
      - .env
    environment:
      MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASS}
      MYSQL_DATABASE: ${DB_NAME}
      POSTGRES_USER: ${DB_USER}
      MYSQL_PASSWORD: ${DB_PASS}

volumes:
  dev-db-data:
  dev-uploads-xlsx:
